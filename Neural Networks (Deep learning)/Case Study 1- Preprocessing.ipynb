{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1baaf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will predict if a customer will buy again for an audiobook company\n",
    "\n",
    "# Columns \n",
    "# ID - is like a name - skipped\n",
    "# Booklength ovr_all- sum of the lengths of the purchase\n",
    "# Booklength average - sum divided by purchases\n",
    "# Price_overall - price is almost always a good predictor\n",
    "# Price_average -\n",
    "# Review - boolean - 1 left a review 0 not (if left a review most likely to buy again)\n",
    "# Review 10/10 - measures the review - for empty -- we substitute it with average of all review. Average will indicate status quo - above average or below average\n",
    "# Total minutes listened\n",
    "# Completion - total minutes listened / book length_overall\n",
    "# Support request - total number of support request(forgotten password etc.)\n",
    "# Last visited minus purchase date - minus first purchase date bigger diff = bigger engagement\n",
    "\n",
    "# Targets - 1 if user coinverted 0 if not\n",
    "\n",
    "# Create a machine learning algorithm that can predict if a customer will buy again\n",
    "# 2 classes - wont buy, or will buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b946acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "#  - Balance the dataset \n",
    "#  - Divide the dataset in training,validation, and test\n",
    "#  - Save the data in a tensor friendly format\n",
    "# Create the machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e01b2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71f5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing # standardizing the input, almost always we standardize the accuracy\n",
    "\n",
    "# Load the data\n",
    "raw_csv_data = np.loadtxt('datasets/case_study_audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "# Extract inputs and targets\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1] # all rows, columns and step\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74318ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "# Counter num of targets that are 1\n",
    "# keep as many as 0s as 1s\n",
    "\n",
    "num_of_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_of_targets:\n",
    "            indices_to_remove.append(i)\n",
    "# will contain the last index to remove in dataset\n",
    "\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0) # axis=0 column\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ff5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the inputs\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors) # all inputs will be standardized\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271d91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation, and test\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count- validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221a3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "np.savez('audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f33f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
